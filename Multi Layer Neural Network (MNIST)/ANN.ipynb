{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\"\"\"\n",
    "Change this code however you want.\n",
    "\"\"\"\n",
    "NUM_CLASSES = 10\n",
    "NUM_HIDDEN = 200\n",
    "INPUT_DIM = 784\n",
    "\n",
    "def load_dataset():\n",
    "    mndata = MNIST('./data/')\n",
    "    X_train, labels_train = map(np.array, mndata.load_training())\n",
    "    # The test labels are meaningless,\n",
    "    # since you're replacing the official MNIST test set with our own test set\n",
    "    X_test, _ = map(np.array, mndata.load_testing())\n",
    "    # Remember to center and normalize the data...\n",
    "    return X_train, labels_train, X_test\n",
    "\n",
    "def sigmoid(x):\n",
    "    from scipy.special import expit\n",
    "    \"Numerically-stable sigmoid function.\"\n",
    "    return expit(x)#1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax of vector x in a numerically stable way.\"\"\"\n",
    "    shiftx = x - np.max(x)\n",
    "    exps = np.exp(shiftx)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def relu(arr):\n",
    "\treturn np.maximum(arr, 0)\n",
    "\n",
    "def drelu(arr):\n",
    "    arr[arr>0] = 1\n",
    "    arr[arr<0] = 0\n",
    "    return arr\n",
    "\n",
    "def one_hot(labels_train):\n",
    "    '''Convert categorical labels 0,1,2,....9 to standard basis vectors in R^{10} '''\n",
    "    return np.eye(NUM_CLASSES)[labels_train]\n",
    "\n",
    "def preprocess(X_train):\n",
    "    from sklearn import preprocessing\n",
    "    return preprocessing.scale(X_train)\n",
    "\n",
    "def predict(W, V, X):\n",
    "    ''' From model and data points, output prediction vectors '''\n",
    "    #forward pass: input to hidden layer\n",
    "    s_h = X @ V.T # N x d+1 x d+1 x h = N x h\n",
    "    h = relu(s_h)\n",
    "    h_bias = np.concatenate((h, np.ones((h.shape[0],1))), axis = 1) #1xh+1\n",
    "\n",
    "    #forward pass: hidden to output layer\n",
    "    s_z = h_bias @ W.T # N x h x h+1 x k = N x k\n",
    "    z = softmax(s_z) # N x k\n",
    "    return np.argmax(z, axis=1)\n",
    "\n",
    "def standardize(X):\n",
    "    return (X - np.mean(X)) / np.std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Deskewing, taken from piazza, https://fsix.github.io/mnist/Deskewing.html\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import math\n",
    "from scipy.ndimage import interpolation\n",
    "\n",
    "def moments(image):\n",
    "    c0,c1 = np.mgrid[:image.shape[0],:image.shape[1]] # A trick in numPy to create a mesh grid\n",
    "    totalImage = np.sum(image) #sum of pixels\n",
    "    m0 = np.sum(c0*image)/totalImage #mu_x\n",
    "    m1 = np.sum(c1*image)/totalImage #mu_y\n",
    "    m00 = np.sum((c0-m0)**2*image)/totalImage #var(x)\n",
    "    m11 = np.sum((c1-m1)**2*image)/totalImage #var(y)\n",
    "    m01 = np.sum((c0-m0)*(c1-m1)*image)/totalImage #covariance(x,y)\n",
    "    mu_vector = np.array([m0,m1]) # Notice that these are \\mu_x, \\mu_y respectively\n",
    "    covariance_matrix = np.array([[m00,m01],[m01,m11]]) # Do you see a similarity between the covariance matrix\n",
    "    return mu_vector, covariance_matrix\n",
    "def deskew(image):\n",
    "    c,v = moments(image)\n",
    "    alpha = v[0,1]/v[0,0]\n",
    "    affine = np.array([[1,0],[alpha,1]])\n",
    "    ocenter = np.array(image.shape)/2.0\n",
    "    offset = c-np.dot(affine,ocenter)\n",
    "    return interpolation.affine_transform(image,affine,offset=offset)\n",
    "\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "def elastic_transform(image, alpha, sigma, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "       Proc. of the International Conference on Document Analysis and\n",
    "       Recognition, 2003.\n",
    "    \"\"\"\n",
    "    assert len(image.shape)==2\n",
    "\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
    "    indices = np.reshape(x+dx, (-1, 1)), np.reshape(y+dy, (-1, 1))\n",
    "    \n",
    "    return map_coordinates(image, indices, order=1).reshape(shape)\n",
    "def deskewAll(X):\n",
    "    currents = []\n",
    "    for i in range(len(X)):\n",
    "        currents.append(deskew(X[i].reshape(28,28)).flatten())\n",
    "    return np.array(currents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Loading the dataset\"\"\"\n",
    "X_train, labels_train, X_test = load_dataset()\n",
    "X_train_original, labels_train_original, X_test_original = X_train, labels_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Deskewing the training and testing sets\"\"\"\n",
    "X_train, X_test = deskewAll(X_train), deskewAll(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Randomly selecting 50000 training points and 10000 validation points\"\"\"\n",
    "train_idx = np.random.choice(X_train.shape[0], X_train.shape[0]-10000, replace=False)\n",
    "val_idx = np.delete(np.arange(X_train.shape[0]), train_idx)\n",
    "labels_train_cut = labels_train[train_idx]\n",
    "labels_val = labels_train[val_idx]\n",
    "y_train = one_hot(labels_train)\n",
    "X_train = X_train.astype(float)\n",
    "X_train_new = preprocess(X_train)\n",
    "X_train_added = np.concatenate((X_train_new, np.ones((X_train_new.shape[0],1))), axis = 1)\n",
    "X_train_cut = X_train_added[train_idx]\n",
    "X_val = X_train_added[val_idx]\n",
    "y_train_cut = y_train[train_idx]\n",
    "y_val = y_train[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon = 0.01 #standard deviation for initial normal distribution\n",
    "plotFreq = 1000 #plot every plotFreq computations\n",
    "def train_sgd(X_train, y_train, y_labels, alpha=0.1, reg=0, \n",
    "              num_iter=15, momentum=False, decay=-10e-6, graph=False, V=None, W=None):\n",
    "    ''' Build a model from X_train -> y_train using stochastic gradient descent '''\n",
    "    if V is None or W is None: #Allows for checkpointing\n",
    "        V = np.random.normal(scale = epsilon, size = (NUM_HIDDEN, INPUT_DIM + 1))\n",
    "        W = np.random.normal(scale = epsilon, size = (NUM_CLASSES, NUM_HIDDEN + 1))\n",
    "    if momentum:\n",
    "        d_W_old = W\n",
    "        d_V_old = V\n",
    "    loss_lst = [] #accumulate losses for analysis\n",
    "    accuracy_lst = [] #accumulate training accuracy for analysis\n",
    "    count = 0\n",
    "    for num in range(num_iter): \n",
    "        #num_iter is the number of times we pass through the ENTIRE datasets\n",
    "        order = np.arange(X_train.shape[0]) \n",
    "        np.random.shuffle(order) #go through dataset in random order\n",
    "        for i in order:\n",
    "            X_i = X_train[i].reshape((1,785)) #current datapoint\n",
    "            y_i = y_train[i].reshape((1,10)) #current labels\n",
    "            \n",
    "            #forward pass: input to hidden layer\n",
    "            s_h = X_i.dot(V.T) # 1 x d+1 x d+1 x h = 1 x h\n",
    "            #h = sigmoid(s_h)\n",
    "            h = relu(s_h) #1 x h\n",
    "            h_bias = np.concatenate((h, np.ones((h.shape[0],1))), axis = 1) #1xh+1\n",
    "            #forward pass: hidden to output layer\n",
    "            s_z = h_bias.dot(W.T) # 1 x h+1 x h+1 x k\n",
    "            z = softmax(s_z) #1 x k\n",
    "            \n",
    "            if graph:\n",
    "                if count % plotFreq == 0:\n",
    "                    #compute loss\n",
    "                    J = -np.sum(y_i*np.log(z))\n",
    "                    loss_lst.append(J)\n",
    "                    #compute accuracy\n",
    "                    pred_labels_train = predict(W, V, X_train)\n",
    "                    accuracy = metrics.accuracy_score(y_labels, pred_labels_train)\n",
    "                    print(\"accuracy:\", accuracy, \"loss\", J)\n",
    "                    accuracy_lst.append(accuracy)\n",
    "                    #reduce learning rate\n",
    "                    alpha = 0.99*alpha\n",
    "\n",
    "            #backprop: output to hidden layer\n",
    "            d_z = z - y_i # 1 x 10\n",
    "            d_W = d_z.T.dot(h_bias) # 10 x 1 x 1 x h+1 = 10 x h+1\n",
    "\n",
    "            #backprop: hidden to input layer\n",
    "            W_dropped = W[:,:-1] #drop the bias that is only relevant to the previous step\n",
    "            #1 x 10 @ 10 x 200 * 1 x 200 .T @ 1 x 785 = 200 x 785\n",
    "            d_V = (d_z.dot(W_dropped) * drelu(s_h)).T.dot(X_i) \n",
    "            \n",
    "            if momentum:\n",
    "                d_W = momentum*d_W_old+(1-momentum)*d_W\n",
    "                d_V = momentum*d_V_old+(1-momentum)*d_V\n",
    "                d_W_old = d_W\n",
    "                d_V_old = d_V\n",
    "                \n",
    "            #update weights\n",
    "            W -= alpha*d_W\n",
    "            V -= alpha*d_V\n",
    "            count += 1\n",
    "    return W, V, loss_lst, accuracy_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for a in [2e-3]:\n",
    "    for dec in [0.99]:\n",
    "        #1e-4, 2e-4, 3e-4, 4e-4, 5e-4, 6e-4, 7e-4, 8e-4, 9e-4, 1e-3, 2e-3, 3e-3, 4e-3, 5e-3, 6e-3, 7e-3, 8e-3\n",
    "        W, V, loss_lst, accuracy_lst = train_sgd(X_train_cut, y_train_cut, labels_train_cut, alpha=a, num_iter=3, momentum=False, decay=dec, graph=True)\n",
    "        pred_labels_train = predict(W, V, X_train_cut)\n",
    "        pred_labels_val = predict(W, V, X_val)\n",
    "        print(\"Stochastic gradient descent w/ alpha=\", a, \"decay=\", dec)\n",
    "        print(\"Train accuracy: {0}\".format(metrics.accuracy_score(labels_train_cut, pred_labels_train)))\n",
    "        print(\"Val accuracy: {0}\".format(metrics.accuracy_score(labels_val, pred_labels_val)))\n",
    "\n",
    "        import matplotlib.pyplot as plt  \n",
    "        %matplotlib inline\n",
    "        loss_arr = np.array(loss_lst)\n",
    "        accuracy_arr = np.array(accuracy_lst)\n",
    "        x = np.arange(len(loss_arr))*plotFreq\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(x, loss_arr, 'y.-')\n",
    "        plt.xlabel('#Iterations')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(x, accuracy_arr, 'r.-')\n",
    "        plt.xlabel('#Iterations')\n",
    "        plt.ylabel('Prediction Accuracy')\n",
    "        plt.savefig(\"plots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test.astype(float)\n",
    "X_test_new = preprocess(X_test)\n",
    "X_test_added = np.concatenate((X_test_new, np.ones((X_test_new.shape[0],1))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Ensemble Learning, Training 5 Identical Neural Networks\"\"\"\n",
    "all_predicts = []\n",
    "all_W = []\n",
    "all_V = []\n",
    "for i in range(5):\n",
    "    #1e-4, 2e-4, 3e-4, 4e-4, 5e-4, 6e-4, 7e-4, 8e-4, 9e-4, 1e-3, 2e-3, 3e-3, 4e-3, 5e-3, 6e-3, 7e-3, 8e-3\n",
    "    W, V, loss_lst, accuracy_lst = train_sgd(X_train_added, y_train, labels_train, alpha=2e-3, num_iter=10, momentum=False, decay=0.99, graph=False)\n",
    "    pred_labels_train = predict(W, V, X_train_added)\n",
    "    pred_labels_test = predict(W, V, X_test_added)\n",
    "    print(\"Stochastic gradient descent w/ alpha=\", a)\n",
    "    print(\"Train accuracy: {0}\".format(metrics.accuracy_score(labels_train, pred_labels_train)))\n",
    "    all_predicts.append(pred_labels_test)\n",
    "    all_W.append(W)\n",
    "    all_V.append(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensemble(all_predicts):\n",
    "    #all_predicts is a list of predictions\n",
    "    #all_predicts is N x 10000\n",
    "    from scipy import stats\n",
    "    return stats.mode(all_predicts, axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_predicts = ensemble(all_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_predict_list = ensemble_predicts.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"CONTEST SUBMISSION\"\"\"\n",
    "import csv\n",
    "\n",
    "f = open(\"ashton_teng_predictions.csv\", 'wt')\n",
    "try:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(('Id', 'Category'))\n",
    "    for i in range(len(pred_labels_test)):\n",
    "        writer.writerow( (i+1, ensemble_predict_list[i]) )\n",
    "finally:\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
