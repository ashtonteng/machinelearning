{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('hw5_data/spam_data/spam_data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = data['training_data']\n",
    "X_test = data['test_data']\n",
    "labels_train = data['training_labels']\n",
    "labels_train = labels_train.reshape((labels_train.shape[1],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5172, 242)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Randomly selecting 4172 training points and 1000 validation points\"\"\"\n",
    "train_idx = np.random.choice(X_train.shape[0], X_train.shape[0]-1000, replace=False)\n",
    "val_idx = np.delete(np.arange(X_train.shape[0]), train_idx)\n",
    "\n",
    "X_train_cut = X_train[train_idx]\n",
    "labels_train_cut = labels_train[train_idx]\n",
    "\n",
    "X_val = X_train[val_idx]\n",
    "labels_val = labels_train[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = DecisionTree(15, 0.1)\n",
    "dt.train(X_train_cut, labels_train_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_labels_train = np.array([dt.predict(X_train_cut[i]) for i in range(X_train_cut.shape[0])])\n",
    "pred_labels_val = np.array([dt.predict(X_val[i]) for i in range(X_val.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9225790987535955\n",
      "Val accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print(\"Train accuracy: {0}\".format(metrics.accuracy_score(labels_train_cut, pred_labels_train)))\n",
    "print(\"Val accuracy: {0}\".format(metrics.accuracy_score(labels_val, pred_labels_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9098994586233565\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For Final Submission\"\"\"\n",
    "dt_final = DecisionTree(15, 0.1)\n",
    "dt_final.train(X_train, labels_train)\n",
    "pred_labels_train = np.array([dt.predict(X_train[i]) for i in range(X_train.shape[0])])\n",
    "pred_labels_test = np.array([dt.predict(X_test[i]) for i in range(X_test.shape[0])])\n",
    "print(\"Train accuracy: {0}\".format(metrics.accuracy_score(labels_train, pred_labels_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"CONTEST SUBMISSION\"\"\"\n",
    "import csv\n",
    "f = open(\"ashton_teng_spam_predictions.csv\", 'wt')\n",
    "try:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(('Id', 'Category'))\n",
    "    for i in range(len(pred_labels_test)):\n",
    "        writer.writerow( (i+1, pred_labels_test[i]) )\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, depth):\n",
    "        self.depth = depth\n",
    "        self.is_leaf = False\n",
    "        self.split_rule = None\n",
    "    def set_left(self, left_node):\n",
    "        self.left = left_node\n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "    def set_right(self, right_node):\n",
    "        self.right = right_node\n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "    def set_split_rule(self, split_rule):\n",
    "        self.split_rule = split_rule\n",
    "    def get_split_rule(self):\n",
    "        return self.split_rule\n",
    "    def set_counts(self, count1, count0):\n",
    "        self.count1 = count1\n",
    "        self.count0 = count0\n",
    "    def get_counts(self):\n",
    "        return self.count1, self.count0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, max_depth, entropy_threshold):\n",
    "        self.root_node = None\n",
    "        self.max_depth = max_depth\n",
    "        self.entropy_threshold = entropy_threshold\n",
    "    \n",
    "    def impurity(self, left_1_count, left_0_count, right_1_count, right_0_count):\n",
    "        \"\"\"count the frequencies of labels on the ”left” and ”right” side of that split. \n",
    "         The method calculates and outputs a scalar value representing the impurity (i.e. the ”badness”)\n",
    "         of the specified split on the input data.\"\"\"\n",
    "        H_left = self.entropy(left_1_count, left_0_count)\n",
    "        H_right = self.entropy(right_1_count, right_0_count)\n",
    "        left_count = left_1_count + left_0_count\n",
    "        right_count = right_1_count + right_0_count\n",
    "        return (left_count*H_left+right_count*H_right) / (left_count+right_count)\n",
    "        \n",
    "    def segmenter(self, data, labels):\n",
    "        \"\"\"finds the best split rule for a Node\"\"\"\n",
    "        #try all split_rules and send histograms to impurity\n",
    "        import sys\n",
    "        best_impurity = sys.maxsize\n",
    "        best_split_rule = None\n",
    "        for feature in range(data.shape[1]):\n",
    "            feature_data = data[:,feature]\n",
    "            best_local_impurity = sys.maxsize\n",
    "            best_local_threshold = None\n",
    "            feature_set = set(feature_data)\n",
    "            if len(feature_set) <= 1:\n",
    "                continue\n",
    "            if len(feature_set) == 2:\n",
    "                feature_set = {sum(feature_set)/2}\n",
    "            else:\n",
    "                feature_set.remove(min(feature_set))\n",
    "            for threshold in feature_set:\n",
    "                left_data, left_labels, right_data, right_labels = self.splitData((feature, threshold), data, labels)\n",
    "                left_1_count = sum(left_labels)\n",
    "                left_0_count = len(left_labels) - left_1_count\n",
    "                right_1_count = sum(right_labels)\n",
    "                right_0_count = len(right_labels) - right_1_count\n",
    "                impurity = self.impurity(left_1_count, left_0_count, right_1_count, right_0_count)\n",
    "                if impurity == 0:\n",
    "                    return (feature, threshold)\n",
    "                if impurity < best_local_impurity:\n",
    "                    best_local_impurity = impurity\n",
    "                    best_local_threshold = threshold\n",
    "            if best_local_impurity < best_impurity:\n",
    "                best_impurity = best_local_impurity\n",
    "                best_split_rule = (feature, best_local_threshold) \n",
    "        return best_split_rule\n",
    "    \n",
    "    def entropy(self, count1, count0):\n",
    "        total = count0+count1\n",
    "        p0 = count0/total\n",
    "        p1 = count1/total\n",
    "        entropy = 0\n",
    "        if p0 > 0:\n",
    "            entropy -= p0*np.log(p0)\n",
    "        if p1 > 0:\n",
    "            entropy -= p1*np.log(p1)\n",
    "        return entropy\n",
    "    \n",
    "    def splitData(self, split_rule, data, labels):\n",
    "        feature, threshold = split_rule\n",
    "        feature_data = data[:,feature]\n",
    "        left_idx = np.argwhere(feature_data < threshold)\n",
    "        left_idx = left_idx.reshape((len(left_idx),))\n",
    "        left_labels = labels[left_idx]\n",
    "        left_data = data[left_idx]\n",
    "        right_idx = np.delete(np.arange(len(feature_data)), left_idx)\n",
    "        right_labels = labels[right_idx]\n",
    "        right_data = data[right_idx]\n",
    "        return left_data, left_labels, right_data, right_labels\n",
    "        \n",
    "    def train(self, data, labels):\n",
    "        \"\"\"Grows a decision tree by constructing nodes.\"\"\"\n",
    "        self.root_node = self.createNodes(data, labels, 0)\n",
    "    \n",
    "    def createNodes(self, data, labels, depth):\n",
    "        \"\"\"Helper function for train\"\"\"\n",
    "        count1 = sum(labels)\n",
    "        count0 = len(labels)-sum(labels)\n",
    "        entropy = self.entropy(count1, count0)\n",
    "        split_rule = self.segmenter(data, labels)\n",
    "        if depth == self.max_depth or entropy < self.entropy_threshold or split_rule == None or len(labels) == 1:\n",
    "            leaf = Node(depth)\n",
    "            leaf.is_leaf = True\n",
    "            leaf.set_counts(count1, count0)\n",
    "            return leaf\n",
    "        else:\n",
    "            node = Node(depth)\n",
    "            left_data, left_labels, right_data, right_labels = self.splitData(split_rule, data, labels)\n",
    "            node.set_split_rule(split_rule)\n",
    "            left_node = self.createNodes(left_data, left_labels, depth+1)\n",
    "            right_node = self.createNodes(right_data, right_labels, depth+1)\n",
    "            node.set_left(left_node)\n",
    "            node.set_right(right_node)\n",
    "            return node\n",
    "            \n",
    "    def predict(self, data):\n",
    "        \"\"\"Given a data point, traverse the tree to find the best label to classify the data point as.\"\"\"\n",
    "        node = self.root_node\n",
    "        while not node.is_leaf:\n",
    "            split_rule = node.get_split_rule()\n",
    "            feature, threshold = split_rule\n",
    "            if data[feature] < threshold:\n",
    "                node = node.get_left()\n",
    "            else:\n",
    "                node = node.get_right()\n",
    "        count1, count0 = node.get_counts()\n",
    "        if count1 > count0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"RANDOM FOREST\"\"\"\n",
    "class RandomForest:\n",
    "    def __init__(self, num_trees, max_height, entropy_threshold, training_drop):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_height = max_height\n",
    "        self.entropy_threshold = entropy_threshold\n",
    "        self.training_drop = training_drop\n",
    "        self.trees = []\n",
    "    def train(self, data, labels):\n",
    "        for tree in range(self.num_trees):\n",
    "            dt = DecisionTree(self.max_height, self.entropy_threshold)\n",
    "            sample_idx = np.random.choice(data.shape[0], data.shape[0]-self.training_drop, replace=True)\n",
    "            data_sample = data[sample_idx]\n",
    "            labels_sample = labels[sample_idx]\n",
    "            dt.train(data_sample, labels_sample)\n",
    "            self.trees.append(dt)\n",
    "    def predict(self, data):\n",
    "        \"\"\"given a data point, outputs ensemble predictions\"\"\"\n",
    "        all_predictions = np.zeros(self.num_trees)\n",
    "        for idx, dt in enumerate(self.trees):\n",
    "            prediction = dt.predict(data)\n",
    "            all_predictions[idx] = prediction\n",
    "        return stats.mode(all_predictions, axis=0)[0].astype(\"int\")\n",
    "    def Adaboost_train(self, data, labels):\n",
    "        \"\"\"trains a series of trees\"\"\"\n",
    "        curr_data = data\n",
    "        curr_labels = labels\n",
    "        for tree in range(self.num_trees):\n",
    "            print(len(curr_data))\n",
    "            dt = DecisionTree(self.max_height, self.entropy_threshold)\n",
    "            dt.train(curr_data, curr_labels)\n",
    "            predictions = np.array([dt.predict(curr_data[i]) for i in range(curr_data.shape[0])])\n",
    "            print(\"accuracy: {0}\".format(metrics.accuracy_score(curr_labels, predictions)))\n",
    "            error_idx = np.argwhere(predictions != curr_labels)\n",
    "            error_idx = error_idx.reshape((len(error_idx)))\n",
    "            curr_data = curr_data[error_idx]\n",
    "            curr_labels = curr_labels[error_idx]\n",
    "            self.trees.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForest(30, 20, 0.01, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.train(X_train_cut, labels_train_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_labels_train = np.array([rf.predict(X_train_cut[i]) for i in range(X_train_cut.shape[0])])\n",
    "pred_labels_val = np.array([rf.predict(X_val[i]) for i in range(X_val.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9369606903163951\n",
      "Val accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print(\"Train accuracy: {0}\".format(metrics.accuracy_score(labels_train_cut, pred_labels_train)))\n",
    "print(\"Val accuracy: {0}\".format(metrics.accuracy_score(labels_val, pred_labels_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20, 15, 0.01, 1000 drop --> 0.897\n",
    "20, 15, 0.01, 1500 drop --> 0.9216, 0.908\n",
    "20, 15, 0.01, 2000 drop --> 0.929, 0.919\n",
    "20, 15, 0.01, 2500 drop --> 0.919, 0.909\n",
    "20, 20, 0.01, 2000 --> 0.936， 0.914\n",
    "30, 20, 0.01, 2000 --> 0.9424, 0.92\n",
    "100, 20, 0.01, 2000 --> 0.9369, 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.94276875483372\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For Final Submission\"\"\"\n",
    "rf_final = RandomForest(30, 20, 0.01, 2000)\n",
    "rf_final.train(X_train, labels_train)\n",
    "pred_labels_train = np.array([rf_final.predict(X_train[i]) for i in range(X_train.shape[0])])\n",
    "pred_labels_test = np.array([rf_final.predict(X_test[i]) for i in range(X_test.shape[0])])\n",
    "print(\"Train accuracy: {0}\".format(metrics.accuracy_score(labels_train, pred_labels_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"CONTEST SUBMISSION\"\"\"\n",
    "import csv\n",
    "f = open(\"ashton_teng_spam_predictions2.csv\", 'wt')\n",
    "try:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(('Id', 'Category'))\n",
    "    for i in range(len(pred_labels_test)):\n",
    "        writer.writerow( (i+1, pred_labels_test[i][0]) )\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
